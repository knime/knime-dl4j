<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="./learn.png" type="Learner" xmlns="http://knime.org/node/v2.8" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
    <name>DL4J Feedforward Learner (Pretraining) (legacy)</name>
    
    <shortDescription>
        Learner for Feedforward Deep Learning Models.
    </shortDescription>
    
    <fullDescription>
        <intro>
        This node performs unsupervised pretraining of a feedforward deep learning model. Thereby, the learning procedure
        can be adjusted using several training methods and parameters, which can be customized in the node dialog. Additionally, 
        the node supplies further methods for regularization, gradient normalization and learning refinements. The learner 
        node automatically adds an output layer to the network configuration, which can be also configured in the node
        dialog. For pretraining the network architecture needs to contain layers which are can be trained unsupervised.
        Such layers are for example an RBM or a Autoencoder. Usually, this node is used together with a classification 
        learner node which performs finetuning of the output layer after the network was pretrained. The output of the 
        node is a pretrained deep learning model.
        
        <p>
            The KNIME Deeplearning4J Integration has been marked as legacy with KNIME Analytics Platform 5.0 and will be deprecated in a future version. 
            If you are using this extension in a production workflow, consider switching to one of the other deep learning integrations available in KNIME Analytics Platform.
		</p>        
        </intro>

        <tab name="Learning Parameters">          
             <option name="Number of Training Iterations">
            The number of parameter updates that will be done on one batch of input data.
            </option>
             <option name="Optimization Algorithm">
            The type of optimization method to use. The following algorithms are available:
            <br/><br/><ul>
				<li>Line Gradient Descent</li>
				<li><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate Gradient Descent</a></li>
				<li>Hessian Free</li>
				<li><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">LBFGS</a></li>
				<li>Stochastic Gradient Descent</li>		
			</ul><br/>
			For Line Gradient Descent, Conjugate Gradient Descent, and LBFGS the maximum number of line search iterations can be specified.
            </option>                      
            <option name="Updater">
            The type of updater to use. These specify how the raw gradients will be modified. If this option is unchecked the node tries
            to use an updater from a previously trained network if available. If not available the default will be used (NESTEROVS). Some
            of the updater types may have additional coefficients which can be adjusted. The The following methods are available:
            <br/><br/><ul>
				<li>SGD</li>
				<li>ADAM (ADAM Mean Decay, ADAM Var Decay)</li>					
				<li>ADADELTA (RHO)</li>				
				<li>NESTEROVS (Momentum, Schedule)<br/>
				Nesterovs Schedule:<br/>
            	Schedule -  Schedule for momentum value change during training. This is specified in the following format:<br/>
            	'iteration':'momentum rate','iteration':'momentum rate'  ... <br/>
           	 	This creates a map, which maps the iteration to the momentum rate that should be used. E.g. '2:0.8' means that 
            	the rate '0.8' should be used in iteration '2'. Leave empty if you do not want to use a schedule.    				
				</li>													
				<li>ADAGRAD</li>
				<li>RMSPROP (RMS Decay)</li>				
			</ul><br/>
			An explanation of these methods and their coefficients can be found 
			<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">here</a>.                                     
            </option>                                              
            <option name="Random Seed">
            The seed value which should be used in order to compare training runs. Any Integer number may be used.
            </option>                                         
            <option name="Regularization">
            The L1 and L2 regularization coefficients.        
            </option>                     
            <option name="Gradient Normalization">
            Gradient normalization strategies. These are applied on raw gradients, before the gradients are passed to the updater. 
            An explanation can be found <a href="http://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/GradientNormalization.html">here</a>.
            <br/><br/><ul>
				<li>Renormalize L2 Per Layer</li>
				<li>Renormalize L2 Per Param Type</li>
				<li>ClipElement Wise Absolute Value</li>
				<li>Clip L2 Per Layer</li>
				<li>Clip L2 Per Param Type</li>
			</ul><br/>
			For 'ClipElement Wise Absolute Value', 'Clip L2 Per Layer', and 'Clip L2 Per Param Type' you can additionally specify a threshold value.	
            </option>                        
        </tab>
        
       <tab name="Global Parameters">               
            <option name="Global Learning Rate">
            The learning rate for the whole network. If not used the learning rate specified in each layer will be used.
            </option>          
            <option name="Global Drop-Out Rate">
            The drop-out rate for the whole network. If not used the drop-out rate specified in each layer will be used.            
            </option>
            <option name="Use Drop-Connect? ">
            Whether to use <a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf">Drop Connect</a>. 
            </option>
            <option name="Global Weight Initialization Strategy">
            The weight initialization strategy to use for the whole network.
            </option>    
            <option name="Global Bias - Learning Rate">
            The bias learning rate for the whole network if you want to use a different learning rate for the bias.
            </option>     
            <option name="Global Bias - Initialization">
            The value to initialize all biases with.
            </option>                 
        </tab>
        
        <tab name="Data Parameters">
            <option name="Batch Size">
            The number of examples used for one minibatch.
            </option>
            <option name="Epochs">
            The number of epochs to train the network, hence the number of training runs on the whole data set.
            </option>           
        </tab>
        
        <tab name="Column Selection">           
            <option name="Feature Column Selection">
            The columns of the input table containing the training data for the network.
            </option>
        </tab>
        
        <tab name="Output Layer Parameter">
            <option name="Number of Output Units">The number of output units of the output layer. This value
            specifies the length of the output vector of the network.</option>
        	<option name="Learning Rate">The learning rate that should be used for this layer.</option>
        	<option name="Weight Initialization Strategy">The strategy which will be used to set the initial weights for this layer.</option>
        	<option name="Loss Function">The type of loss function that should be used for this layer.</option>
        </tab>

    </fullDescription>
    <ports>
        <inPort index="0" name="Deep Learning Model">Finished configuration of a deep learning network.</inPort>
        <inPort index="1" name="Data Table">Data table containing training data.</inPort>
        
        <outPort index="0" name="Deep Learning Model">Trained Deep Learning Model</outPort>      
    </ports>     
    <views>
        <view index="0" name="Learning Status">
        Shows information about the current learning run. Has an option for early stopping of training. If training
        is stopped before the last epoch the model will be saved in the current status.
        </view>
    </views>    
</knimeNode>

<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="./learn.png" deprecated="true" type="Learner" xmlns="http://knime.org/node/v2.8" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
    <name>DL4J Feedforward Learner (legacy)</name>
    
    <shortDescription>
        Learner for Feedforward Deep Learning Models.
    </shortDescription>
    
    <fullDescription>
        <intro>
        This node supplies means to learn the network configuration specified by the Deep Learning Model. Thereby,
        the model can be either trained supervised or unsupervised using several training methods like Stochastic Gradient
        Descent. The output layer of the network, which can be configured in the node dialog, will be automatically added by this node. 
        Additionally, the node supplies further methods for regularization, gradient normalization and learning refinements.
        In order to learn the network, inputs will be automatically converted into a network understandable vector format. For the model 
        input there are two options. If the supplied model is untrained it will be trained normally by the learner. If the model was 
        trained by a previous learner the node will try to use the network parameters of the trained model to initialise the parameters 
        of the new network for the new training run, because the network configuration can be changed between learner nodes. This way 
        methods like <a href="https://en.wikipedia.org/wiki/Inductive_transfer">Transfer Learning</a> can be implemented. The output 
        of the node is a learned Deep Learning Model containing the original configuration and tuned network weights and biases.
        
        <p>
            The KNIME Deeplearning4J Integration has been marked as legacy with KNIME Analytics Platform 5.0 and will be deprecated in a future version. 
            If you are using this extension in a production workflow, consider switching to one of the other deep learning integrations available in KNIME Analytics Platform.
		</p>        
        </intro>

        <tab name="Learning Parameters">
            <option name="Training Mode">
            Whether to do supervised or unsupervised training.
            <ul>
				<li>SUPERVISED - label column needs to be specified</li>
				<li>UNSUPERVISED - label column can be omitted</li>				
			</ul>	
            </option>
            <option name="Use Seed">
            Whether to use a seed value for training. Used to make different learning runs comparable. If the same seed was used and 
            the configuration didn't change, the results will the same between learning runs.
            </option>
            <option name="Seed">
            The seed value which should be used. Any Integer number may be used.
            </option>
            <option name="Number of Training Iterations">
            The number of parameter updates that will be done on one batch of input data.
            </option>
            <option name="Optimization Algorithm">
            The type of optimization method to use. The following algorithms are available:
            <ul>
				<li>LINE_GRADIENT_DESCENT - normal gradient descent</li>
				<li><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">CONJUGATE_GRADIENT</a></li>
				<li>HESSIAN_FREE</li>
				<li><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">LBFGS</a></li>
				<li>STOCHASTIC_GRADIENT_DESCENTT - gradient descent using minibatches</li>		
			</ul>	
            </option>
            <option name="Do Backpropagation">
            Whether to do backpropagation. If this option is chosen the learner will perform supervised training using the specified 
            techniques and hyper parameters.
            </option>
            <option name="Do Pretraining">
            Whether to to pretreaining. If this option is chosen the learner will perform unsupervised pretraining (Contrastive Divergence)
             of the network parameters. This option is only applicable for Restricted Boltzmann Machines and Autoencoders. 
            </option>
            <option name="Do Finetuning">
            Whether to to finetuning. If this option is chosen the learner will perform supervised finetuning of the network parameters.
            </option>
            <option name="Use Pretrained Updater">
            Whether to use a pretrained updater of a trained model. Some updaters contain a history of previous gradients, hence, it can be 
            specified if a supplied updater should be taken or a new should be created. This option will only take effect if Deep Learning 
            Model supplied at the input was previously trained and contains a saved updater.
            </option>
            <option name="Updater Type">
            The type of updater to use. These specify how the raw gradients will be modified. If a pretrained updater is used this option
            will be ignored. The The following methods are available:
            <ul>
				<li>SGD</li>
				<li>ADAM</li>
				<li>ADADELTA</li>
				<li>NESTEROVS</li>
				<li>ADAGRAD</li>
				<li>ADAGRAD</li>
				<li>RMSPROP</li>
			</ul>	
            </option>
            <option name="Use Regularization">
            Whether to use regularization techniques to prevent overfitting.
            </option>
            <option name="L1 Regularization Coefficient">
            Strength of L1 regularization.
            </option>
            <option name="L2 Regularization Coefficient">
            Strength of L2 regularization.
            </option>
            <option name="Use Gradient Normalization">
            Whether to use gradient normalization.
            </option>
            <option name="Gradient Normalization Strategy">
            Gradient normalization strategies. These are applied on raw gradients, before the gradients are passed to the updater. 
            An explanation can be found at:<br/>
            <a href="http://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/GradientNormalization.html">http://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/GradientNormalization.html</a>
             <ul>
				<li>RenormalizeL2PerLayer</li>
				<li>RenormalizeL2PerParamType</li>
				<li>ClipElementWiseAbsoluteValue</li>
				<li>ClipL2PerLayer</li>
				<li>ClipL2PerParamType</li>
			</ul>	
            </option>
            <option name="Gradient Normalization Threshold">
            	Threshold value for gradient normalization.
            </option>
            <option name="Use Momentum">
            Whether to use momentum.
            </option>
            <option name="Momentum Rate">
            Rate of influence of the momentum term.
            </option>
            <option name="Momentum After">
            Schedule for momentum value change during training. This is specified in the following format:<br/>
            'iteration':'momentum rate','iteration':'momentum rate'  ... <br/>
            This creates a map, which maps the iteration to the momentum rate that should be used. E.g. '2:0.8' means that 
            the rate '0.8' should be used in iteration '2'. Leave empty if you do not want to use a schedule.
            </option>
            <option name="Use Drop Connect">
            Whether to use <a href="http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf">Drop Connect</a>. 
            </option>                       
        </tab>
        <tab name="Global Parameters">
            <option name="Use Global Learning Rate">
            Whether to overwrite the learning rates specified in the layers of the network for all layers.
            </option>
            <option name="Global Learning Rate">
            The learning rate to use for all layers.
            </option>
            <option name="Use Global Drop Out Rate">
            Whether to overwrite the drop out rates specified in the layers of the network for all layers.
            </option>
            <option name="Global Drop Out Rate">
            The drop out rate to use for all layers.
            </option>
            <option name="Use Global Weight Initialization Strategy">
            Whether to overwrite the weight initialization strategy specified in the layers of the network for all layers.
            </option>
            <option name="Global Weight Initilialization Strategy">
            The weight initialization strategy to use for all layers.
            </option>
        </tab>
        <tab name="Data Parameters">
            <option name="Batch Size">
            The number of examples used for one minibatch.
            </option>
            <option name="Epochs">
            The number of epochs to train the network, hence the number of training runs on the whole data set.
            </option>
            <option name="Size of Input Image">
            If the input table contains images the dimensionality of the images needs to be specified.
            This value needs to be three numbers separated by a comma specifying the dimension sizes of
            the images (size x,size y,number of channels). E.g. 64,64,3 
            </option>
        </tab>
        <tab name="Column Selection">
            <option name="Label Column">
            The column of the input table containing labels for supervised learning.
            </option>
            <option name="Input Column Selection">
            The columns of the input table containing the training data for the network.
            </option>
        </tab>
        <tab name="Output Layer Parameter">
            <option name="Number of Output Units">The number of outputs for this layer. For supervised training this value is 
        	determined automatically, hence it is not possible to set it. For unsupervised training this value specifies the
        	number of neurons in the output layer.</option>
        	<option name="Learning Rate">The learning rate that should be used for this layer.</option>
        	<option name="Weight Initialization Strategy">The strategy which will be used to set the initial weights for this layer.</option>
        	<option name="Loss Function">The type of loss function that should be used for this layer.</option>
<option name="Activation Function">The type of activation function that should be used for this layer.</option>
        </tab>

    </fullDescription>
    <ports>
        <inPort index="0" name="Deep Learning Model">Finished configuration of a deep learning network.</inPort>
        <inPort index="1" name="Data Table">Data table containing training data.</inPort>
        
        <outPort index="0" name="Deep Learning Model">Trained Deep Learning Model</outPort>      
    </ports>     
    <views>
        <view index="0" name="Learning Status">
        Shows information about the current learning run. Has an option for early stopping of training. If training
        is stopped before the last epoch the model will be saved in the current status.
        </view>
    </views>    
</knimeNode>
